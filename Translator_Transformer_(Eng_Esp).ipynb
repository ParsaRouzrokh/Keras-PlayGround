{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZLyHFLzbHZ2Sf8bH/wzmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParsaRouzrokh/Keras-PlayGround/blob/main/Translator_Transformer_(Eng_Esp).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jm7PW0JKR91",
        "outputId": "097dd6d5-fd86-49d0-df61-e39f7d2100b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-31 19:42:05--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.79.128, 108.177.119.128, 108.177.126.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.79.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   2.52M  3.82MB/s    in 0.7s    \n",
            "\n",
            "2023-08-31 19:42:06 (3.82 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Dataset Download\n",
        "import tensorflow as tf\n",
        "keras = tf.keras\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from random import shuffle\n",
        "import string\n",
        "import re\n",
        "\n",
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading Dataset\n",
        "\n",
        "with open (\"spa-eng/spa.txt\") as f:\n",
        "  lines = f.read().split('\\n')[:-1]\n",
        "\n",
        "# As mentioned in README of the dataset, the last record is empty."
      ],
      "metadata": {
        "id": "UVKwTOC3Xx8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "for line in lines[:42000]:\n",
        "  eng,esp = line.split('\\t')\n",
        "  # Adding start and end tokens to our target texts\n",
        "  esp = '[start] '+ esp + ' [end]'\n",
        "\n",
        "  # Building lists including pairs of eng and esp texts\n",
        "  pairs.append((eng,esp))\n",
        "pairs[0]"
      ],
      "metadata": {
        "id": "qQE_YRc0MGY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e3cf5a3-7780-4c1d-d9a1-f26b625844bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Go.', '[start] Ve. [end]')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle and split\n",
        "\n",
        "from random import shuffle\n",
        "shuffle(pairs)\n",
        "pairs_length = len (pairs)\n",
        "val_length = test_length =  int(0.15 * pairs_length)\n",
        "train_length = pairs_length - 2 * (val_length)\n",
        "train_pairs = pairs[:train_length]\n",
        "val_pairs = pairs[train_length:train_length + val_length]\n",
        "test_pairs = pairs[train_length + val_length:]"
      ],
      "metadata": {
        "id": "Zjefz6mROo86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalization\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace('[','') # Needed for Tokens!\n",
        "strip_chars = strip_chars.replace(']','') # Needed for Tokens!\n",
        "strip_chars # !\"#$%&\\'()*+,-./:;<=>?@\\\\^_`{|}~¿\n",
        "\n",
        "def standard (input_str): # in kar faqat baraye spaniaii bayad anjam beshe yani target !\n",
        "  lowercase = tf.strings.lower(input_str)\n",
        "\n",
        "  # Using re.escape to consider special characters as literal ones!\n",
        "  stn = tf.strings.regex_replace(lowercase , f\"[{re.escape(strip_chars)}]\" , \"\")\n",
        "\n",
        "  return stn"
      ],
      "metadata": {
        "id": "v77aaVmXdRKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization\n",
        "\n",
        "# We are using the first 15000 of the most common used words in our corpus.\n",
        "vocab_size = 15000\n",
        "\n",
        "# We are setting the lengths of our input and output literal texts (not tokens) to 20!\n",
        "seq_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length= seq_length)\n",
        "\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = \"int\",\n",
        "\n",
        "    # Since we are using encoder_decoder here and we are using esp texts for \\\n",
        "    # inputs and outputs, the out_seq_length must me 21 here!\n",
        "    # input_esp:21 => all tokens without [end]\n",
        "    # target_esp:21 => all tokens without [start]\n",
        "    output_sequence_length = seq_length + 1,\n",
        "    standardize = standard\n",
        ")\n",
        "\n",
        "# Passing the corpuses of our vocabs to our vectorization layers.\n",
        "\n",
        "full_train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "full_train_esp_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(full_train_eng_texts)\n",
        "target_vectorization.adapt(full_train_esp_texts)\n"
      ],
      "metadata": {
        "id": "K_PkosN7QAdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating our datasets with the correct format for encoder_decoder\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng,spa):\n",
        "  eng = source_vectorization(eng)\n",
        "  spa = target_vectorization(spa)\n",
        "  return({\"english\" : eng , \"spanish\" : spa[:,:-1]}, spa[:,1:])\n",
        "\n",
        "def make_dataset (pairs):\n",
        "  eng_texts , spa_texts = zip(*pairs)\n",
        "  eng_texts = list(eng_texts)\n",
        "  spa_texts = list(spa_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts,spa_texts))\n",
        "  dataset = dataset.batch (batch_size)\n",
        "  dataset = dataset.map(format_dataset , num_parallel_calls = 4)\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset (train_pairs)\n",
        "val_ds = make_dataset (val_pairs)"
      ],
      "metadata": {
        "id": "n_L_OhF5aPaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity Check Here!\n",
        "\n",
        "for inputs,targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape : {inputs['english'].shape}\")\n",
        "  print(f\"inputs['spanish'].shape : {inputs['spanish'].shape}\")\n",
        "  print(f\"targets.shape : {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kte9JfIk-7j7",
        "outputId": "17810a92-3f83-41e3-9c99-ebc4374c5cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape : (64, 20)\n",
            "inputs['spanish'].shape : (64, 20)\n",
            "targets.shape : (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets build our Transformer:\n",
        "# 1) PositionalEmbedding 2) TransformerEncoder 3) TransformerDecoder\n",
        "\n",
        "\n",
        "# 1) PositionalEmbedding:\n",
        "class PositionalEmbedding (layers.Layer):\n",
        "  def __init__(self, seq_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(input_dim = input_dim , output_dim = output_dim)\n",
        "    self.position_embeddings = layers.Embedding (input_dim = seq_length , output_dim = output_dim)\n",
        "    self.seq_length = seq_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start = 0 , limit = length , delta = 1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return (embedded_tokens + embedded_positions)\n",
        "\n",
        "  # Mask Padding with 0 and 1:\n",
        "  def compute_mask (self, inputs, mask = None):\n",
        "    return (tf.math.not_equal(inputs,0))\n",
        "\n",
        "\n",
        "# Updating the config of our model:\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"output_dim\" : self.output_dim,\n",
        "        \"seq_length\" : self.seq_length,\n",
        "        \"input_dim\" : self.input_dim\n",
        "    })\n",
        "    return config\n",
        "\n",
        "# Transformer Encoder:\n",
        "\n",
        "class TransformerEncoder (layers.Layer):\n",
        "  def __init__(self , embed_dim , dense_dim , num_heads , **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = layers.MultiHeadAttention(num_heads = num_heads , key_dim = embed_dim)\n",
        "    self.dense_proj = keras.Sequential([\n",
        "        layers.Dense(dense_dim , activation = \"relu\"),\n",
        "        layers.Dense(embed_dim)\n",
        "    ])\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "\n",
        "  def get_config (self):\n",
        "    config = super().get_config\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"dense_dim\": self.dense_dim,\n",
        "        \"num_heads\": self.num_heads\n",
        "      })\n",
        "    return config\n",
        "\n",
        "  def call(self,inputs, mask = None):\n",
        "    if mask is not None:\n",
        "      mask = mask[:,tf.newaxis,:]\n",
        "    attention_out = self.attention(\n",
        "        query = inputs,\n",
        "        key = inputs,\n",
        "        value = inputs,\n",
        "        attention_mask = mask\n",
        "      )\n",
        "    # Multi_head self_attention for encoder part\n",
        "    proj_input = self.layernorm_1(attention_out + inputs)\n",
        "    proj_output = self.dense_proj (proj_input)\n",
        "\n",
        "    return(self.layernorm_2(proj_input+proj_output))\n",
        "\n",
        "# Transformer Decoder\n",
        "class TransformerDecoder (layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = layers.MultiHeadAttention(num_heads = num_heads , key_dim = embed_dim)\n",
        "    self.attention_2 = layers.MultiHeadAttention(num_heads = num_heads , key_dim = embed_dim)\n",
        "    self.dense_proj = keras.Sequential([\n",
        "        layers.Dense(dense_dim , activation = \"relu\"),\n",
        "        layers.Dense(embed_dim)\n",
        "    ])\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "    self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "    # Required for passing the custom mask defined in the encoder part, to the decoder part!\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def get_config (self):\n",
        "    config = super().get_config\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"dense_dim\": self.dense_dim,\n",
        "        \"num_heads\": self.num_heads\n",
        "    })\n",
        "    return config\n",
        "\n",
        "\n",
        "  # Since in our transformer architecture, the model is inspecting our whole esp \\\n",
        "  # inputs and targets at the same time, we must create padding masks for these 2!\n",
        "\n",
        "\n",
        "  def casual_attention_mask(self,inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size , sequence_length = input_shape[0],input_shape[1]\n",
        "    i = tf.range(sequence_length)[:,tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    mask = tf.cast (i >= j , dtype = \"int32\") # Now the lower half of our 20*20 matrix is 1 , rest = 0!\n",
        "    mask = tf.reshape(mask,(1,sequence_length,sequence_length))\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size,-1),tf.constant([1,1],dtype = tf.int32)], axis = 0)\n",
        "    return tf.tile(mask,mult)\n",
        "\n",
        "\n",
        "  def call(self,inputs,encoder_outputs, mask = None):\n",
        "    casual_mask = self.casual_attention_mask(inputs)\n",
        "    if mask is not None:\n",
        "      padding_mask = tf.cast(mask[:,tf.newaxis,:] , dtype =\"int32\")\n",
        "      # choosing the lowest value between these two based on our encoder part!\n",
        "      padding_mask = tf.minimum(padding_mask,casual_mask)\n",
        "    attention_out1 = self.attention_1(\n",
        "            query = inputs,\n",
        "            value = inputs,\n",
        "            key = inputs,\n",
        "            attention_mask = casual_mask\n",
        "        )\n",
        "    attention_out1 = self.layernorm_1(attention_out1 + inputs)\n",
        "    attention_out2 = self.attention_2(\n",
        "            query = attention_out1,\n",
        "            value = encoder_outputs,\n",
        "            key = encoder_outputs,\n",
        "            attention_mask = padding_mask # Prevent Spoilers!\n",
        "        )\n",
        "    attention_out2 = self.layernorm_2(attention_out2 + attention_out1)\n",
        "    proj_out = self.dense_proj(attention_out2)\n",
        "\n",
        "    return (self.layernorm_3(proj_out + attention_out2))"
      ],
      "metadata": {
        "id": "_uyi62H3UlLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# END TO END:\n",
        "\n",
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape = (None,), dtype = \"int64\" , name = \"english\")\n",
        "x = PositionalEmbedding(seq_length, vocab_size, embed_dim) (encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim , num_heads) (x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape = (None,) , dtype =\"int64\", name = \"spanish\")\n",
        "x = PositionalEmbedding(seq_length, vocab_size, embed_dim) (decoder_inputs)\n",
        "x = TransformerDecoder (embed_dim, dense_dim , num_heads)(x,encoder_outputs)\n",
        "decoder_outputs = layers.Dense(vocab_size , activation = \"softmax\")(x)\n",
        "Transformer = keras.Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
        "Transformer.compile(optimizer = \"rmsprop\" , loss = \"sparse_categorical_crossentropy\" , metrics = [\"accuracy\"])\n",
        "\n",
        "history = Transformer.fit(train_ds , epochs = 30 , validation_data = val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyVD8_iTld1s",
        "outputId": "600b629a-eb52-428b-a425-6462f8dc7c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "460/460 [==============================] - 53s 87ms/step - loss: 3.7043 - accuracy: 0.4833 - val_loss: 2.9105 - val_accuracy: 0.5586\n",
            "Epoch 2/30\n",
            "460/460 [==============================] - 30s 65ms/step - loss: 2.4758 - accuracy: 0.6046 - val_loss: 2.4645 - val_accuracy: 0.6105\n",
            "Epoch 3/30\n",
            "460/460 [==============================] - 30s 66ms/step - loss: 1.9335 - accuracy: 0.6698 - val_loss: 2.2034 - val_accuracy: 0.6457\n",
            "Epoch 4/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 1.5680 - accuracy: 0.7168 - val_loss: 2.0620 - val_accuracy: 0.6661\n",
            "Epoch 5/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 1.3014 - accuracy: 0.7521 - val_loss: 2.0604 - val_accuracy: 0.6712\n",
            "Epoch 6/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 1.0934 - accuracy: 0.7818 - val_loss: 2.0278 - val_accuracy: 0.6754\n",
            "Epoch 7/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.9244 - accuracy: 0.8067 - val_loss: 2.1306 - val_accuracy: 0.6682\n",
            "Epoch 8/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.7866 - accuracy: 0.8284 - val_loss: 2.1005 - val_accuracy: 0.6762\n",
            "Epoch 9/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.6738 - accuracy: 0.8484 - val_loss: 2.2090 - val_accuracy: 0.6665\n",
            "Epoch 10/30\n",
            "460/460 [==============================] - 31s 68ms/step - loss: 0.5804 - accuracy: 0.8677 - val_loss: 2.2624 - val_accuracy: 0.6694\n",
            "Epoch 11/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.5057 - accuracy: 0.8839 - val_loss: 2.2585 - val_accuracy: 0.6694\n",
            "Epoch 12/30\n",
            "460/460 [==============================] - 31s 68ms/step - loss: 0.4479 - accuracy: 0.8949 - val_loss: 2.3652 - val_accuracy: 0.6678\n",
            "Epoch 13/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.3980 - accuracy: 0.9047 - val_loss: 2.3579 - val_accuracy: 0.6668\n",
            "Epoch 14/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.3687 - accuracy: 0.9106 - val_loss: 2.3308 - val_accuracy: 0.6734\n",
            "Epoch 15/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.3420 - accuracy: 0.9156 - val_loss: 2.4083 - val_accuracy: 0.6701\n",
            "Epoch 16/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.3209 - accuracy: 0.9201 - val_loss: 2.4383 - val_accuracy: 0.6715\n",
            "Epoch 17/30\n",
            "460/460 [==============================] - 31s 68ms/step - loss: 0.3034 - accuracy: 0.9235 - val_loss: 2.4478 - val_accuracy: 0.6744\n",
            "Epoch 18/30\n",
            "460/460 [==============================] - 31s 66ms/step - loss: 0.2897 - accuracy: 0.9263 - val_loss: 2.5073 - val_accuracy: 0.6686\n",
            "Epoch 19/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.2823 - accuracy: 0.9278 - val_loss: 2.5341 - val_accuracy: 0.6686\n",
            "Epoch 20/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.2696 - accuracy: 0.9314 - val_loss: 2.5620 - val_accuracy: 0.6711\n",
            "Epoch 21/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.2606 - accuracy: 0.9342 - val_loss: 2.5412 - val_accuracy: 0.6740\n",
            "Epoch 22/30\n",
            "460/460 [==============================] - 33s 71ms/step - loss: 0.2548 - accuracy: 0.9349 - val_loss: 2.5769 - val_accuracy: 0.6722\n",
            "Epoch 23/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.2496 - accuracy: 0.9364 - val_loss: 2.5530 - val_accuracy: 0.6782\n",
            "Epoch 24/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.2406 - accuracy: 0.9382 - val_loss: 2.5921 - val_accuracy: 0.6723\n",
            "Epoch 25/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.2345 - accuracy: 0.9403 - val_loss: 2.5823 - val_accuracy: 0.6725\n",
            "Epoch 26/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.2290 - accuracy: 0.9411 - val_loss: 2.6530 - val_accuracy: 0.6715\n",
            "Epoch 27/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.2223 - accuracy: 0.9431 - val_loss: 2.6010 - val_accuracy: 0.6767\n",
            "Epoch 28/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.2183 - accuracy: 0.9440 - val_loss: 2.7463 - val_accuracy: 0.6690\n",
            "Epoch 29/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.2152 - accuracy: 0.9451 - val_loss: 2.6422 - val_accuracy: 0.6760\n",
            "Epoch 30/30\n",
            "460/460 [==============================] - 31s 67ms/step - loss: 0.2098 - accuracy: 0.9465 - val_loss: 2.6633 - val_accuracy: 0.6747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# It's always better not to trust any metric in translations except yourself!\n",
        "\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "\n",
        "# Creating Dictionary of Spanish word indices and the words themselves!\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)),spa_vocab))\n",
        "\n",
        "max_decoded_length = 20\n",
        "\n",
        "def decode_sequence (input_sentence):\n",
        "  tokenized_input = source_vectorization([input_sentence])\n",
        "  decoded_sentence = \"[start]\"\n",
        "  for i in range (max_decoded_length):\n",
        "    tokenized_output = target_vectorization([decoded_sentence])[:,:-1]\n",
        "    next_token_pred = Transformer.predict([tokenized_input,tokenized_output])\n",
        "    sampled_token_index = np.argmax(next_token_pred[:,i,:])\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "    if sampled_token == \"end\":\n",
        "      break\n",
        "  return str(decoded_sentence).split(\"[start]\")[1].split(\"[end]\")[0].strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "4vQ4gLqUgmZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"for _ in range(20):\n",
        "  input_sen = random.choice(full_train_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sen)\n",
        "  print(decode_sequence(input_sen))\"\"\"\n",
        "print(decode_sequence(\"what's your name\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMEj_YjxEkmn",
        "outputId": "2e8b222a-a4bc-4d44-8113-92b8481d40c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "cómo te llamas\n"
          ]
        }
      ]
    }
  ]
}